{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Prophet Time Series Project (Updated for Cultus Submission Requirements)\n",
        "---------------------------------------------------------------------\n",
        "This script focuses exclusively on the Facebook/Meta Prophet forecasting model and\n",
        "implements a rigorous hyperparameter optimization and validation workflow required\n",
        "by the project rubric. It produces the following deliverables required for submission:\n",
        "\n",
        "- A realistic synthetic daily dataset with seasonality, holidays, changepoints, noise.\n",
        "- A systematic hyperparameter tuning pipeline for Prophet (grid search and optional\n",
        "  Bayesian search via Optuna if available) using rolling-origin cross-validation.\n",
        "- A baseline comparison using Holt-Winters Exponential Smoothing and an optional\n",
        "  SARIMA baseline (if pmdarima is available).\n",
        "- A text-based report (Markdown) that details the hyperparameter search space,\n",
        "  the validation strategy, and a quantitative comparison of Prophet vs baselines.\n",
        "- All results and intermediate CSVs saved into an `output/` folder for submission.\n",
        "\n",
        "Important: This file intentionally **does not** include any Neural ODEs, LSTMs,\n",
        "or unrelated models. It focuses on Prophet and the classical baselines required.\n",
        "\n",
        "Usage:\n",
        "    python prophet_time_series_project.py --generate-data\n",
        "    python prophet_time_series_project.py --run-all\n",
        "\n",
        "Requirements (pip install):\n",
        "    prophet numpy pandas scikit-learn statsmodels matplotlib optuna pmdarima\n",
        "\n",
        "Note on packages:\n",
        "- `prophet` (or `fbprophet` depending on your environment) is required.\n",
        "- `optuna` is optional but recommended for faster Bayesian hyperparameter search.\n",
        "- `pmdarima` is optional for SARIMA baseline.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Model imports\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "except Exception:\n",
        "    Prophet = None\n",
        "\n",
        "try:\n",
        "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "except Exception:\n",
        "    ExponentialSmoothing = None\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "except Exception:\n",
        "    optuna = None\n",
        "\n",
        "try:\n",
        "    import pmdarima as pm\n",
        "except Exception:\n",
        "    pm = None\n",
        "\n",
        "# ----------------------------- Metrics -------------------------------------\n",
        "\n",
        "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    return mean_squared_error(y_true, y_pred, squared=False)\n",
        "\n",
        "\n",
        "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "\n",
        "def mase(y_true: np.ndarray, y_pred: np.ndarray, y_train: np.ndarray, seasonal_period: int = 1) -> float:\n",
        "    \"\"\"Mean Absolute Scaled Error (MASE).\n",
        "    Uses seasonal naive with given seasonal_period as the denominator if seasonal_period>1,\n",
        "    otherwise uses naive lag-1.\n",
        "    \"\"\"\n",
        "    n = len(y_train)\n",
        "    if n < 2:\n",
        "        return np.inf\n",
        "    if seasonal_period > 1 and n > seasonal_period:\n",
        "        denom = np.mean(np.abs(y_train[seasonal_period:] - y_train[:-seasonal_period]))\n",
        "    else:\n",
        "        denom = np.mean(np.abs(y_train[1:] - y_train[:-1]))\n",
        "    num = np.mean(np.abs(y_true - y_pred))\n",
        "    return num / denom if denom != 0 else np.inf\n",
        "\n",
        "# ------------------------- Data generation ---------------------------------\n",
        "\n",
        "def generate_synthetic_daily_series(start_date: str = '2018-01-01', periods: int = 3 * 365, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Generate a synthetic daily time series with:\n",
        "    - piecewise linear trend with changepoints\n",
        "    - yearly seasonality\n",
        "    - weekly seasonality (weekday effect)\n",
        "    - holiday spikes\n",
        "    - noise and outliers\n",
        "\n",
        "    Returns (df, holidays_df) where df has ['ds','y'] and holidays_df has ['ds','holiday']\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    dates = pd.date_range(start=start_date, periods=periods, freq='D')\n",
        "    t = np.arange(periods)\n",
        "\n",
        "    # Base linear trend\n",
        "    trend = 0.02 * t\n",
        "    # Introduce changepoints\n",
        "    cp1 = int(periods * 0.28)\n",
        "    cp2 = int(periods * 0.6)\n",
        "    trend[cp1:] += 0.12 * (t[cp1:] - cp1)\n",
        "    trend[cp2:] -= 0.09 * (t[cp2:] - cp2)\n",
        "\n",
        "    # Yearly seasonality\n",
        "    yearly = 8.0 * np.sin(2 * np.pi * t / 365.25)\n",
        "\n",
        "    # Weekly effect: weekend bump\n",
        "    weekday = np.array([0.0 if d.weekday() < 5 else 4.0 for d in dates])\n",
        "\n",
        "    # Holidays: select some dates and apply multi-day window effect\n",
        "    rng = np.random.default_rng(seed)\n",
        "    num_holidays = max(6, int(periods / 180))\n",
        "    holiday_indices = rng.choice(periods, size=num_holidays, replace=False)\n",
        "    holiday_effect = np.zeros(periods)\n",
        "    for hi in holiday_indices:\n",
        "        start = max(0, hi - 1)\n",
        "        end = min(periods, hi + 2)\n",
        "        holiday_effect[start:end] += rng.normal(15, 4)\n",
        "\n",
        "    # Noise and outliers\n",
        "    noise = np.random.normal(0, 2.0, size=periods)\n",
        "    outliers = np.zeros(periods)\n",
        "    outlier_positions = rng.choice(periods, size=int(periods * 0.004), replace=False)\n",
        "    outliers[outlier_positions] = rng.normal(30, 12, size=outlier_positions.shape[0])\n",
        "\n",
        "    y = 45 + trend + yearly + weekday + holiday_effect + noise + outliers\n",
        "\n",
        "    df = pd.DataFrame({'ds': dates, 'y': y})\n",
        "    holidays = pd.DataFrame({'ds': dates[holiday_indices], 'holiday': ['synthetic_holiday'] * len(holiday_indices)})\n",
        "    holidays = holidays.sort_values('ds').reset_index(drop=True)\n",
        "    return df.reset_index(drop=True), holidays\n",
        "\n",
        "# ------------------------- CV splits --------------------------------------\n",
        "\n",
        "def rolling_origin_splits(n_obs: int, initial: int, horizon: int, step: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"Generate rolling-origin (expanding window) train/test index splits.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_obs: total number of observations\n",
        "    initial: initial training size (in observations)\n",
        "    horizon: test horizon for each fold\n",
        "    step: shift size for each fold\n",
        "    \"\"\"\n",
        "    splits = []\n",
        "    start = initial\n",
        "    while start + horizon <= n_obs:\n",
        "        train_idx = np.arange(0, start)\n",
        "        test_idx = np.arange(start, start + horizon)\n",
        "        splits.append((train_idx, test_idx))\n",
        "        start += step\n",
        "    return splits\n",
        "\n",
        "# ----------------------- Prophet grid search -------------------------------\n",
        "\n",
        "def grid_search_prophet(df: pd.DataFrame, holidays: pd.DataFrame, param_grid: Dict[str, List[Any]],\n",
        "                        cv_initial: int = 365, cv_horizon: int = 90, cv_step: int = 90,\n",
        "                        seasonal_period: int = 7, verbose: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"Perform grid search on Prophet hyperparameters using rolling-origin CV.\n",
        "\n",
        "    Returns a dict with 'best_params' and 'results_df' (DataFrame of results).\n",
        "    \"\"\"\n",
        "    if Prophet is None:\n",
        "        raise ImportError(\"Prophet package not found. Install with `pip install prophet`\")\n",
        "\n",
        "    import itertools\n",
        "    keys = list(param_grid.keys())\n",
        "    combos = list(itertools.product(*(param_grid[k] for k in keys)))\n",
        "    results = []\n",
        "\n",
        "    n = df.shape[0]\n",
        "    splits = rolling_origin_splits(n, initial=cv_initial, horizon=cv_horizon, step=cv_step)\n",
        "    if verbose:\n",
        "        print(f\"Running grid search with {len(combos)} combinations and {len(splits)} CV folds...\")\n",
        "\n",
        "    for combo in combos:\n",
        "        params = dict(zip(keys, combo))\n",
        "        fold_metrics = []\n",
        "        for train_idx, test_idx in splits:\n",
        "            train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "            test_df = df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "            m = Prophet(\n",
        "                changepoint_prior_scale=params.get('changepoint_prior_scale', 0.05),\n",
        "                seasonality_mode=params.get('seasonality_mode', 'additive'),\n",
        "                weekly_seasonality=params.get('weekly_seasonality', True),\n",
        "                yearly_seasonality=params.get('yearly_seasonality', True)\n",
        "            )\n",
        "            # Set seasonality prior via attribute where supported\n",
        "            if 'seasonality_prior_scale' in params:\n",
        "                try:\n",
        "                    m.seasonality_prior_scale = params['seasonality_prior_scale']\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            if holidays is not None and not holidays.empty:\n",
        "                m.holidays = holidays\n",
        "\n",
        "            # Fit and predict\n",
        "            m.fit(train_df)\n",
        "            future = test_df[['ds']].copy()\n",
        "            forecast = m.predict(future)\n",
        "            y_true = test_df['y'].values\n",
        "            y_pred = forecast['yhat'].values\n",
        "\n",
        "            fold_metrics.append({\n",
        "                'rmse': rmse(y_true, y_pred),\n",
        "                'mae': mae(y_true, y_pred),\n",
        "                'mase': mase(y_true, y_pred, train_df['y'].values, seasonal_period=seasonal_period)\n",
        "            })\n",
        "\n",
        "        avg_rmse = np.mean([fm['rmse'] for fm in fold_metrics])\n",
        "        avg_mae = np.mean([fm['mae'] for fm in fold_metrics])\n",
        "        avg_mase = np.mean([fm['mase'] for fm in fold_metrics])\n",
        "\n",
        "        result_row = {**params, 'rmse': avg_rmse, 'mae': avg_mae, 'mase': avg_mase}\n",
        "        results.append(result_row)\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values(['mase', 'rmse']).reset_index(drop=True)\n",
        "    best = results_df.iloc[0].to_dict()\n",
        "    best_params = {k: best[k] for k in keys}\n",
        "    return {'best_params': best_params, 'results_df': results_df}\n",
        "\n",
        "# ---------------------- Optuna Bayesian search (optional) -------------------\n",
        "\n",
        "def bayes_search_prophet(df: pd.DataFrame, holidays: pd.DataFrame, n_trials: int = 40,\n",
        "                         cv_initial: int = 365, cv_horizon: int = 90, cv_step: int = 90,\n",
        "                         seasonal_period: int = 7) -> Dict[str, Any]:\n",
        "    \"\"\"Use Optuna to tune Prophet hyperparameters. Returns best_params and study object.\n",
        "    Optuna must be installed; otherwise this function will raise an informative error.\n",
        "    \"\"\"\n",
        "    if optuna is None:\n",
        "        raise ImportError(\"Optuna is not installed. Install with `pip install optuna` or use grid_search_prophet instead.\")\n",
        "    if Prophet is None:\n",
        "        raise ImportError(\"Prophet package not found. Install with `pip install prophet`\")\n",
        "\n",
        "    n = df.shape[0]\n",
        "    splits = rolling_origin_splits(n, initial=cv_initial, horizon=cv_horizon, step=cv_step)\n",
        "\n",
        "    def objective(trial):\n",
        "        cps = trial.suggest_loguniform('changepoint_prior_scale', 0.0005, 0.5)\n",
        "        sps = trial.suggest_categorical('seasonality_prior_scale', [0.1, 0.5, 1.0, 3.0, 5.0, 10.0])\n",
        "        mode = trial.suggest_categorical('seasonality_mode', ['additive', 'multiplicative'])\n",
        "        weekly = trial.suggest_categorical('weekly_seasonality', [True, False])\n",
        "        yearly = trial.suggest_categorical('yearly_seasonality', [True, False])\n",
        "\n",
        "        fold_mases = []\n",
        "        for train_idx, test_idx in splits:\n",
        "            train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "            test_df = df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "            m = Prophet(changepoint_prior_scale=cps, seasonality_mode=mode,\n",
        "                        weekly_seasonality=weekly, yearly_seasonality=yearly)\n",
        "            try:\n",
        "                m.seasonality_prior_scale = sps\n",
        "            except Exception:\n",
        "                pass\n",
        "            if holidays is not None and not holidays.empty:\n",
        "                m.holidays = holidays\n",
        "            m.fit(train_df)\n",
        "            forecast = m.predict(test_df[['ds']])\n",
        "            fold_mases.append(mase(test_df['y'].values, forecast['yhat'].values, train_df['y'].values, seasonal_period=seasonal_period))\n",
        "\n",
        "        return float(np.mean(fold_mases))\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "    best_params = study.best_params\n",
        "    return {'best_params': best_params, 'study': study}\n",
        "\n",
        "# ------------------------- Model fit & forecast ----------------------------\n",
        "\n",
        "def fit_prophet_and_forecast(df_train: pd.DataFrame, df_future: pd.DataFrame, params: Dict[str, Any], holidays: pd.DataFrame = None) -> pd.DataFrame:\n",
        "    if Prophet is None:\n",
        "        raise ImportError(\"Prophet package not found. Install with `pip install prophet`\")\n",
        "\n",
        "    m = Prophet(\n",
        "        changepoint_prior_scale=params.get('changepoint_prior_scale', 0.05),\n",
        "        seasonality_mode=params.get('seasonality_mode', 'additive'),\n",
        "        weekly_seasonality=params.get('weekly_seasonality', True),\n",
        "        yearly_seasonality=params.get('yearly_seasonality', True)\n",
        "    )\n",
        "    if 'seasonality_prior_scale' in params:\n",
        "        try:\n",
        "            m.seasonality_prior_scale = params['seasonality_prior_scale']\n",
        "        except Exception:\n",
        "            pass\n",
        "    if holidays is not None and not holidays.empty:\n",
        "        m.holidays = holidays\n",
        "\n",
        "    m.fit(df_train)\n",
        "    forecast = m.predict(df_future)\n",
        "    # Keep only relevant columns\n",
        "    return forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
        "\n",
        "# ------------------------- Baseline models --------------------------------\n",
        "\n",
        "def fit_holt_winters_and_forecast(train_series: pd.Series, forecast_index: pd.DatetimeIndex, seasonal_periods: int = 7) -> pd.DataFrame:\n",
        "    if ExponentialSmoothing is None:\n",
        "        raise ImportError(\"statsmodels ExponentialSmoothing not available. Install statsmodels.\")\n",
        "    model = ExponentialSmoothing(train_series, trend='add', seasonal='add', seasonal_periods=seasonal_periods)\n",
        "    fitted = model.fit(optimized=True)\n",
        "    yhat = fitted.forecast(len(forecast_index))\n",
        "    return pd.DataFrame({'ds': forecast_index, 'yhat': yhat.values})\n",
        "\n",
        "\n",
        "def fit_sarima_and_forecast(train_series: pd.Series, forecast_index: pd.DatetimeIndex, seasonal_period: int = 7) -> pd.DataFrame:\n",
        "    \"\"\"Optional SARIMA baseline using pmdarima auto_arima. Returns DataFrame with ds and yhat.\n",
        "    \"\"\"\n",
        "    if pm is None:\n",
        "        raise ImportError(\"pmdarima not available. Install with `pip install pmdarima` to enable SARIMA baseline.\")\n",
        "    model = pm.auto_arima(train_series, seasonal=True, m=seasonal_period, error_action='ignore', suppress_warnings=True)\n",
        "    yhat = model.predict(n_periods=len(forecast_index))\n",
        "    return pd.DataFrame({'ds': forecast_index, 'yhat': yhat})\n",
        "\n",
        "# ------------------------- Reporting --------------------------------------\n",
        "\n",
        "def save_report(output_dir: str, config: Dict[str, Any], tuning_results: pd.DataFrame, best_params: Dict[str, Any], prophet_eval: Dict[str, Any], hw_eval: Dict[str, Any], sarima_eval: Dict[str, Any] = None) -> str:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    report_path = os.path.join(output_dir, 'prophet_tuning_report.md')\n",
        "\n",
        "    lines = []\n",
        "    lines.append('# Prophet Hyperparameter Tuning Report')\n",
        "    lines.append('')\n",
        "    lines.append('## Configuration')\n",
        "    lines.append('')\n",
        "    lines.append('```json')\n",
        "    lines.append(json.dumps(config, indent=2))\n",
        "    lines.append('```')\n",
        "    lines.append('')\n",
        "\n",
        "    lines.append('## Hyperparameter Search Space')\n",
        "    lines.append('')\n",
        "    lines.append('The grid or search space used for Prophet hyperparameter tuning:')\n",
        "    lines.append('')\n",
        "    lines.append(tuning_results.head(50).to_markdown(index=False))\n",
        "    lines.append('')\n",
        "\n",
        "    lines.append('## Best Parameters')\n",
        "    lines.append('')\n",
        "    lines.append('```json')\n",
        "    lines.append(json.dumps(best_params, indent=2))\n",
        "    lines.append('```')\n",
        "    lines.append('')\n",
        "\n",
        "    lines.append('## Evaluation on Holdout')\n",
        "    lines.append('')\n",
        "    lines.append('### Prophet Metrics')\n",
        "    lines.append('')\n",
        "    lines.append(pd.DataFrame([prophet_eval]).to_markdown(index=False))\n",
        "    lines.append('')\n",
        "    lines.append('### Holt-Winters Metrics')\n",
        "    lines.append('')\n",
        "    lines.append(pd.DataFrame([hw_eval]).to_markdown(index=False))\n",
        "    lines.append('')\n",
        "    if sarima_eval is not None:\n",
        "        lines.append('### SARIMA Metrics')\n",
        "        lines.append('')\n",
        "        lines.append(pd.DataFrame([sarima_eval]).to_markdown(index=False))\n",
        "        lines.append('')\n",
        "\n",
        "    lines.append('## Validation Strategy')\n",
        "    lines.append('')\n",
        "    lines.append('* Rolling-origin (expanding window) cross-validation on training data')\n",
        "    lines.append('* Initial training window: {}'.format(config.get('cv_initial')))\n",
        "    lines.append('* Forecast horizon per fold: {}'.format(config.get('cv_horizon')))\n",
        "    lines.append('* Step between folds: {}'.format(config.get('cv_step')))\n",
        "    lines.append('')\n",
        "\n",
        "    lines.append('## Notes and Recommendations')\n",
        "    lines.append('')\n",
        "    lines.append('- The primary metric used for model selection is MASE (scale-free).')\n",
        "    lines.append('- Secondary metrics: RMSE and MAE.')\n",
        "    lines.append('- For production, consider using Optuna for faster Bayesian tuning and enabling parallelization.')\n",
        "\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write('\n",
        "'.join(lines))\n",
        "\n",
        "    return report_path\n",
        "\n",
        "# ------------------------- Pipeline ---------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class ProjectConfig:\n",
        "    start_date: str = '2018-01-01'\n",
        "    periods: int = 3 * 365\n",
        "    seed: int = 42\n",
        "    cv_initial: int = 365\n",
        "    cv_horizon: int = 90\n",
        "    cv_step: int = 90\n",
        "    output_dir: str = 'output'\n",
        "    holdout_days: int = 180\n",
        "    seasonal_period: int = 7\n",
        "\n",
        "\n",
        "def run_pipeline(config: ProjectConfig, use_bayesian: bool = False, bayes_trials: int = 40) -> Dict[str, Any]:\n",
        "    os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Generate synthetic data\n",
        "    df, holidays = generate_synthetic_daily_series(start_date=config.start_date, periods=config.periods, seed=config.seed)\n",
        "    df.to_csv(os.path.join(config.output_dir, 'simulated_series.csv'), index=False)\n",
        "    holidays.to_csv(os.path.join(config.output_dir, 'simulated_holidays.csv'), index=False)\n",
        "\n",
        "    # 2. Split into training (for tuning) and holdout\n",
        "    holdout_days = config.holdout_days\n",
        "    train_df = df.iloc[:-holdout_days].reset_index(drop=True)\n",
        "    holdout_df = df.iloc[-holdout_days:].reset_index(drop=True)\n",
        "\n",
        "    # 3. Define hyperparameter grid for Prophet (clear, documented)\n",
        "    param_grid = {\n",
        "        'changepoint_prior_scale': [0.001, 0.01, 0.05, 0.1],\n",
        "        'seasonality_prior_scale': [1.0, 5.0, 10.0],\n",
        "        'seasonality_mode': ['additive', 'multiplicative'],\n",
        "        'weekly_seasonality': [True, False],\n",
        "        'yearly_seasonality': [True, False]\n",
        "    }\n",
        "\n",
        "    config_dict = {\n",
        "        'cv_initial': config.cv_initial,\n",
        "        'cv_horizon': config.cv_horizon,\n",
        "        'cv_step': config.cv_step,\n",
        "        'param_grid_keys': list(param_grid.keys())\n",
        "    }\n",
        "\n",
        "    # 4. Hyperparameter tuning (grid or Bayesian)\n",
        "    if use_bayesian and optuna is not None:\n",
        "        print('Running Bayesian optimization via Optuna...')\n",
        "        bayes_res = bayes_search_prophet(train_df, holidays, n_trials=bayes_trials,\n",
        "                                         cv_initial=config.cv_initial, cv_horizon=config.cv_horizon, cv_step=config.cv_step,\n",
        "                                         seasonal_period=config.seasonal_period)\n",
        "        best_params = bayes_res['best_params']\n",
        "        # Optuna uses floats/ints; try to make compatible keys\n",
        "        # Add default keys if missing\n",
        "        for k in ['weekly_seasonality', 'yearly_seasonality', 'seasonality_mode']:\n",
        "            if k not in best_params:\n",
        "                best_params[k] = True\n",
        "        tuning_results_df = pd.DataFrame([best_params])\n",
        "    else:\n",
        "        print('Running grid search over Prophet hyperparameters...')\n",
        "        tune_res = grid_search_prophet(train_df, holidays, param_grid,\n",
        "                                       cv_initial=config.cv_initial, cv_horizon=config.cv_horizon, cv_step=config.cv_step,\n",
        "                                       seasonal_period=config.seasonal_period)\n",
        "        best_params = tune_res['best_params']\n",
        "        tuning_results_df = tune_res['results_df']\n",
        "\n",
        "    tuning_results_df.to_csv(os.path.join(config.output_dir, 'prophet_tuning_results.csv'), index=False)\n",
        "\n",
        "    # 5. Fit optimized Prophet on full training set and forecast holdout\n",
        "    prophet_forecast = fit_prophet_and_forecast(train_df, holdout_df[['ds']].copy(), best_params, holidays=holidays)\n",
        "    prophet_forecast.to_csv(os.path.join(config.output_dir, 'prophet_holdout_forecast.csv'), index=False)\n",
        "\n",
        "    prophet_eval = {\n",
        "        'rmse': rmse(holdout_df['y'].values, prophet_forecast['yhat'].values),\n",
        "        'mae': mae(holdout_df['y'].values, prophet_forecast['yhat'].values),\n",
        "        'mase': mase(holdout_df['y'].values, prophet_forecast['yhat'].values, train_df['y'].values, seasonal_period=config.seasonal_period)\n",
        "    }\n",
        "    pd.DataFrame([prophet_eval]).to_csv(os.path.join(config.output_dir, 'prophet_holdout_metrics.csv'), index=False)\n",
        "\n",
        "    # 6. Baseline: Holt-Winters\n",
        "    try:\n",
        "        hw_forecast = fit_holt_winters_and_forecast(train_df['y'], holdout_df['ds'], seasonal_periods=config.seasonal_period)\n",
        "        hw_forecast.to_csv(os.path.join(config.output_dir, 'hw_holdout_forecast.csv'), index=False)\n",
        "        hw_eval = {\n",
        "            'rmse': rmse(holdout_df['y'].values, hw_forecast['yhat'].values),\n",
        "            'mae': mae(holdout_df['y'].values, hw_forecast['yhat'].values),\n",
        "            'mase': mase(holdout_df['y'].values, hw_forecast['yhat'].values, train_df['y'].values, seasonal_period=config.seasonal_period)\n",
        "        }\n",
        "        pd.DataFrame([hw_eval]).to_csv(os.path.join(config.output_dir, 'hw_holdout_metrics.csv'), index=False)\n",
        "    except Exception as e:\n",
        "        hw_eval = None\n",
        "        print('Warning: Holt-Winters baseline not run:', str(e))\n",
        "\n",
        "    # Optional SARIMA baseline if pmdarima available\n",
        "    sarima_eval = None\n",
        "    if pm is not None:\n",
        "        try:\n",
        "            sarima_forecast = fit_sarima_and_forecast(train_df['y'], holdout_df['ds'], seasonal_period=config.seasonal_period)\n",
        "            sarima_forecast.to_csv(os.path.join(config.output_dir, 'sarima_holdout_forecast.csv'), index=False)\n",
        "            sarima_eval = {\n",
        "                'rmse': rmse(holdout_df['y'].values, sarima_forecast['yhat'].values),\n",
        "                'mae': mae(holdout_df['y'].values, sarima_forecast['yhat'].values),\n",
        "                'mase': mase(holdout_df['y'].values, sarima_forecast['yhat'].values, train_df['y'].values, seasonal_period=config.seasonal_period)\n",
        "            }\n",
        "            pd.DataFrame([sarima_eval]).to_csv(os.path.join(config.output_dir, 'sarima_holdout_metrics.csv'), index=False)\n",
        "        except Exception as e:\n",
        "            print('Warning: SARIMA baseline failed:', str(e))\n",
        "\n",
        "    # 7. Generate text-based report for submission\n",
        "    report_path = save_report(config.output_dir, config_dict, tuning_results_df, best_params, prophet_eval, hw_eval or {}, sarima_eval)\n",
        "\n",
        "    # 8. Optional plotting (non-blocking)\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(pd.concat([train_df, holdout_df])['ds'], pd.concat([train_df, holdout_df])['y'], label='observed')\n",
        "        plt.plot(prophet_forecast['ds'], prophet_forecast['yhat'], label='Prophet')\n",
        "        if hw_eval is not None:\n",
        "            plt.plot(hw_forecast['ds'], hw_forecast['yhat'], label='Holt-Winters')\n",
        "        if sarima_eval is not None:\n",
        "            plt.plot(sarima_forecast['ds'], sarima_forecast['yhat'], label='SARIMA')\n",
        "        plt.legend()\n",
        "        plt.title('Observed vs Forecasts (Holdout)')\n",
        "        plt.xlabel('ds')\n",
        "        plt.ylabel('y')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(config.output_dir, 'forecast_comparison.png'))\n",
        "        plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    summary = {\n",
        "        'best_params': best_params,\n",
        "        'prophet_eval': prophet_eval,\n",
        "        'hw_eval': hw_eval,\n",
        " 'sarima_eval': sarima_eval,\n",
        "        'tuning_results_csv': os.path.join(config.output_dir, 'prophet_tuning_results.csv'),\n",
        "        'prophet_forecast_csv': os.path.join(config.output_dir, 'prophet_holdout_forecast.csv'),\n",
        "        'hw_forecast_csv': os.path.join(config.output_dir, 'hw_holdout_forecast.csv') if hw_eval is not None else None,\n",
        "        'report_md': report_path,\n",
        "        'dataset_csv': os.path.join(config.output_dir, 'simulated_series.csv')\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "# ------------------------- CLI --------------------------------------------\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Prophet Time Series Project - Cultus Submission Ready')\n",
        "    parser.add_argument('--generate-data', action='store_true', help='Only generate and save the synthetic dataset')\n",
        "    parser.add_argument('--run-all', action='store_true', help='Run full pipeline including tuning and evaluation')\n",
        "    parser.add_argument('--output-dir', type=str, default='output', help='Output directory')\n",
        "    parser.add_argument('--bayes', action='store_true', help='Use Bayesian tuning (Optuna) instead of grid search')\n",
        "    parser.add_argument('--trials', type=int, default=40, help='Number of Optuna trials when using Bayesian tuning')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config = ProjectConfig(output_dir=args.output_dir)\n",
        "\n",
        "    if args.generate_data:\n",
        "        df, holidays = generate_synthetic_daily_series(start_date=config.start_date, periods=config.periods, seed=config.seed)\n",
        "        os.makedirs(config.output_dir, exist_ok=True)\n",
        "        df.to_csv(os.path.join(config.output_dir, 'simulated_series.csv'), index=False)\n",
        "        holidays.to_csv(os.path.join(config.output_dir, 'simulated_holidays.csv'), index=False)\n",
        "        print(f\"Saved simulated dataset to {os.path.join(config.output_dir, 'simulated_series.csv')}\")\n",
        "        return\n",
        "\n",
        "    if args.run_all:\n",
        "        print('Running full pipeline... this may take time depending on tuning configuration.')\n",
        "        summary = run_pipeline(config, use_bayesian=args.bayes, bayes_trials=args.trials)\n",
        "        print('Pipeline complete. Summary:')\n",
        "        for k, v in summary.items():\n",
        "            print(f'  {k}: {v}')\n",
        "        return\n",
        "\n",
        "    parser.print_help()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "z9Cmu1iGCU_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}